# -*- coding: utf-8 -*-
"""Movie_Recommendation_System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dgLIJM2lQixBBgf9x4-jEetSEbAiShoz

# Load data
"""

import pandas as pd
import numpy as np
df1 = pd.read_csv("https://github.com/bean0418/python2024/blob/main/nadocoding/data/tmdb_5000_credits.csv?raw=true")
df2 = pd.read_csv("https://github.com/bean0418/python2024/blob/main/nadocoding/data/tmdb_5000_movies.csv?raw=true")

# data가 있는 url + "?raw=true"를 통해서 바로 불러오기

df1.head()

df2.head()

"""# 1. Demographic Filtering"""

df1['title'].equals(df2['title'])

# 두 데이터의 값이 같다.
# -> df1의 title은 없애고 id를 기준으로 두 데이터프레임을 병합한다.

df1.columns

df1.columns = ['id', 'title', 'cast', 'crew']
df1.head()

# rename columns (movie_id -> id)

df1[['id', 'title', 'cast', 'crew']]

df = df2.merge(df1[['id', 'cast', 'crew']], on = 'id')
df.head()

# df의 id, cast, crew를 id를 기준으로 merge (title은 제외)

C = df["vote_average"].mean()
C

m = df["vote_count"].quantile(0.9) # 상위 10%
m

q_movies = df2.copy().loc[df["vote_count"] >= m]
q_movies.shape

q_movies["vote_count"].sort_values() # 상위 10%가 1838이었으므로 데이터가 잘 인덱싱된 것을 확인.

def weighted_rating(x, m=m, C=C):
    v = x['vote_count']
    R = x['vote_average']
    return (v / (v + m) * R) + (m / (m + v) * C)

q_movies['score'] = q_movies.apply(weighted_rating, axis = 1) # q_movies의 모든 데이터들에 대해 weighted_rating 함수를 적용시킴.
q_movies.head()

q_movies = q_movies.sort_values("score", ascending = False) # 오름차순 = false -> 내림차순
q_movies[['title', 'vote_count', 'vote_average', 'score']].head(10)

# 기존 데이터에서 영화의 인기도를 측정하는 측도인 popularity를 기준으로 시각화

pop= df.sort_values('popularity', ascending=False)
import matplotlib.pyplot as plt
plt.figure(figsize=(12,4))

plt.barh(pop['title'].head(10),pop['popularity'].head(10), align='center',
        color='skyblue')
plt.gca().invert_yaxis()
plt.xlabel("Popularity")
plt.title("Popular Movies")

"""# 2. Content Based Filtering"""

df['overview'].head()

# 목적: overview의 글을 분석하여 유사 콘텐츠를 추천하는 방식

"""## Bag Of Words - BOW
### I am a boy
### I am a girl
### I(2), am(2), a(2), boy(1), girl(1)로 분리 (횟수)

###        I   am  a   boy   girl
### 문장1:  1    1  1    1     0
### 문장2:  1    1  1    0     1
### 문서 100개, 모든 문서에서 나온 단어가 10,000개라면
### 100 x 10,000 의 행렬으로 표현

1. TfidVectorizer (TF-IDF 기반의 벡터화)
-> 중요도가 낮은 a, the와 같은 관사에 가중치를 적게 지정함으로써
학습의 정확도를 높임
2. CountVectorizer
"""

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(stop_words = "english")

from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
ENGLISH_STOP_WORDS

# stop_words = 중요도가 낮은 단어들

df['overview'].isnull().values.any()

df['overview'] = df['overview'].fillna("")

df['overview'].isnull().values.any()

tfidf_matrix = tfidf.fit_transform(df['overview'])
tfidf_matrix.shape

# 4803개의 문서들이 20978개의 단어들로 이루어져있다. stop_words 제외

from sklearn.metrics.pairwise import linear_kernel

cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)
cosine_sim # cosine similarity -> symmetric matrix

cosine_sim.shape

indices = pd.Series(df.index, index = df['title']).drop_duplicates()
indices

indices["Spectre"]

df.iloc[[2]]

"""### 영화의 제목을 입력하면 코사인 유사도를 통해 가장 유사도가 높은 상위 10개 영화 목록 반환하는 함수"""

def get_recommendations(title, cosine_sim = cosine_sim):
    pass

test_idx = indicies[]